import requests
from bs4 import BeautifulSoup
import os
import logging
from urllib.parse import urljoin, urlparse, parse_qs
import time
import json

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
BASE_URL = "http://publication.pravo.gov.ru"
MAIN_PAGE_URL = f"{BASE_URL}/documents/block/president"
PDF_DIR = "pdfs"
LOG_FILE = "download.log"
TIMEOUT = 45
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

def setup_environment():
    os.makedirs(PDF_DIR, exist_ok=True)
    logging.info(f"–†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")

def debug_save_response(response, page_num):
    debug_dir = "debug_responses"
    os.makedirs(debug_dir, exist_ok=True)
    filename = os.path.join(debug_dir, f"page_{page_num}.html")
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(response.text)
    logging.debug(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω –æ—Ç–≤–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num} –≤ {filename}")

def get_total_pages(session):
    try:
        logging.info("–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü...")
        response = session.get(
            MAIN_PAGE_URL,
            params={"index": 1, "pageSize": 200},
            timeout=TIMEOUT
        )
        response.raise_for_status()
        
        debug_save_response(response, 1)
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –≤ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
        pagination = soup.find('ul', class_='pagination')
        if pagination:
            last_page_link = pagination.find_all('a')[-2]
            last_page_url = last_page_link.get('href')
            parsed = urlparse(last_page_url)
            return int(parse_qs(parsed.query)['index'][0])
        
        # –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ —Ç–µ–∫—Å—Ç–∞ "–ü–æ—Å–ª–µ–¥–Ω—è—è"
        last_page_text = soup.find('a', string='–ü–æ—Å–ª–µ–¥–Ω—è—è')
        if last_page_text:
            last_page_url = last_page_text.get('href')
            parsed = urlparse(last_page_url)
            return int(parse_qs(parsed.query)['index'][0])
        
        # –ú–µ—Ç–æ–¥ 3: –ü–æ–¥—Å—á–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        total_docs = len(soup.find_all("div", class_="infoindocumentlist"))
        if total_docs == 200:
            logging.warning("–í–æ–∑–º–æ–∂–Ω–æ –±–æ–ª—å—à–µ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 87")
            return 87
        
        return 1
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü: {str(e)}")
        return 1

def process_page(session, page_num):
    logging.info(f"‚ñ∂Ô∏è –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}")
    try:
        params = {"index": page_num, "pageSize": 200}
        response = session.get(
            MAIN_PAGE_URL,
            params=params,
            timeout=TIMEOUT
        )
        response.raise_for_status()
        
        debug_save_response(response, page_num)
        
        if response.status_code != 200:
            logging.error(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –∫–æ–¥ {response.status_code}")
            return 0, 0, 0
            
        soup = BeautifulSoup(response.text, 'html.parser')
        blocks = soup.find_all("div", class_="infoindocumentlist")
        
        if not blocks:
            logging.warning(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return 0, 0, 0
            
        stats = {'downloaded': 0, 'errors': 0, 'skipped': 0}
        
        for block in blocks:
            time.sleep(0.5)
            try:
                # –ü–æ–∏—Å–∫ –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                number_tag = block.find("span", class_="info-data")
                if not number_tag:
                    logging.debug("–ù–µ –Ω–∞–π–¥–µ–Ω –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞")
                    continue
                pub_number = number_tag.text.strip()
                
                # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–∫–∏ –Ω–∞ PDF
                pdf_link = block.find("a", class_="documents-item-file")
                if not pdf_link or not pdf_link.get('href'):
                    logging.debug(f"–î–æ–∫—É–º–µ–Ω—Ç {pub_number}: –Ω–µ—Ç —Å—Å—ã–ª–∫–∏")
                    continue
                
                pdf_url = urljoin(BASE_URL, pdf_link['href'])
                filename = os.path.join(PDF_DIR, f"{pub_number}.pdf")
                
                if os.path.exists(filename):
                    stats['skipped'] += 1
                    logging.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π: {pub_number}")
                    continue
                
                # –ó–∞–≥—Ä—É–∑–∫–∞ PDF
                pdf_response = session.get(
                    pdf_url,
                    stream=True,
                    timeout=TIMEOUT
                )
                
                if pdf_response.status_code == 200:
                    with open(filename, 'wb') as f:
                        for chunk in pdf_response.iter_content(chunk_size=8192):
                            f.write(chunk)
                    stats['downloaded'] += 1
                    logging.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {pub_number}")
                else:
                    stats['errors'] += 1
                    logging.error(f"‚ùå –û—à–∏–±–∫–∞ {pdf_response.status_code}: {pub_number}")
                
            except Exception as e:
                stats['errors'] += 1
                logging.error(f"–û—à–∏–±–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}")
                continue
        
        logging.info(f"‚úîÔ∏è –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {stats}")
        return stats
        
    except Exception as e:
        logging.error(f"üî• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {str(e)}")
        return {'downloaded': 0, 'errors': 1, 'skipped': 0}

def main():
    setup_environment()
    session = requests.Session()
    session.headers.update(HEADERS)
    
    total_pages = get_total_pages(session)
    logging.info(f"–í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_pages}")
    
    total_stats = {
        'downloaded': 0,
        'errors': 0,
        'skipped': 0,
        'processed_pages': 0
    }
    
    for page_num in range(1, total_pages + 1):
        try:
            page_stats = process_page(session, page_num)
            total_stats['downloaded'] += page_stats['downloaded']
            total_stats['errors'] += page_stats['errors']
            total_stats['skipped'] += page_stats['skipped']
            total_stats['processed_pages'] += 1
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            with open('progress.json', 'w') as f:
                json.dump(total_stats, f)
            
            time.sleep(3)
            
        except KeyboardInterrupt:
            logging.info("‚èπ –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            break
            
        except Exception as e:
            logging.error(f"üí£ –§–∞—Ç–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
            continue
    
    logging.info("\nüìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    logging.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_stats['processed_pages']}/{total_pages}")
    logging.info(f"–£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–æ: {total_stats['downloaded']}")
    logging.info(f"–û—à–∏–±–æ–∫: {total_stats['errors']}")
    logging.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ: {total_stats['skipped']}")
    logging.info(f"–§–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ: {len(os.listdir(PDF_DIR))}")

if __name__ == "__main__":
    main()
